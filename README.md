# reading

## Data architecture

* [Emerging Architectures for Modern Data Infrastructure](https://future.a16z.com/emerging-architectures-modern-data-infrastructure)

## Machine learning

### Metrics

* [Precision & Recall](https://mlu-explain.github.io/precision-recall/)

### Neural network

* [Why is my validation loss lower than my training loss?](https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/)

## To read

* [How to use Learning Curves to Diagnose Machine Learning Model Performance](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
* [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
* [Gradient Boosting In Classification: Not a Black Box Anymore!](https://blog.paperspace.com/gradient-boosting-for-classification/)
* [Greedy Function Approximation: A Gradient Boosting Machine](https://jerryfriedman.su.domains/ftp/trebst.pdf)
* [Drift in Machine Learning](https://towardsdatascience.com/drift-in-machine-learning-e49df46803a)
* [Data Drift vs. Concept Drift: What Are the Main Differences?](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/)
* [Why You Should Care About Data and Concept Drift](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift)
* [Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/)
